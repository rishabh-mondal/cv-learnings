{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folders = [\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Facebook\",\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Telegram\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # Define a lightweight Vision Transformer model\n",
    "# class LightweightViT(nn.Module):\n",
    "#     def __init__(self, num_classes=4, embed_dim=128, num_heads=4, num_layers=4, mlp_dim=256):\n",
    "#         super(LightweightViT, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.patch_size = 16\n",
    "#         self.num_patches = (224 // self.patch_size) ** 2\n",
    "\n",
    "#         # Patch embedding\n",
    "#         self.patch_embed = nn.Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "#         # Transformer encoder layers\n",
    "#         self.encoder_layers = nn.ModuleList([\n",
    "#             nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(nn.Sequential(*self.encoder_layers), num_layers=num_layers)\n",
    "\n",
    "#         # Classification head\n",
    "#         self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Patch embedding\n",
    "#         x = self.patch_embed(x)  # Shape: [batch_size, embed_dim, num_patches, patch_size]\n",
    "#         x = x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "\n",
    "#         # Transformer encoder\n",
    "#         x = self.transformer_encoder(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "\n",
    "#         # Classification head\n",
    "#         x = x.mean(dim=1)  # Aggregate patch embeddings\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Define a custom dataset class\n",
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, image_paths, labels, transform=None):\n",
    "#         self.image_paths = image_paths\n",
    "#         self.labels = labels\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "#         label = self.labels[idx]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, label\n",
    "\n",
    "# # Set paths and labels\n",
    "# image_folders = [\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Facebook\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Telegram\",\n",
    "# ]\n",
    "# image_paths = []\n",
    "# labels = []\n",
    "\n",
    "# # Assume folder1 has label 0, folder2 has label 1, etc.\n",
    "# for i, folder in enumerate(image_folders):\n",
    "#     for img_name in os.listdir(folder):\n",
    "#         img_path = os.path.join(folder, img_name)\n",
    "#         image_paths.append(img_path)\n",
    "#         labels.append(i)\n",
    "\n",
    "# # Define transformations and dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# dataset = ImageDataset(image_paths, labels, transform=transform)\n",
    "# data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Instantiate and move the model to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = LightweightViT(num_classes=len(image_folders))\n",
    "# model.to(device)\n",
    "\n",
    "# # Define the optimizer and loss function\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# model.train()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels in tqdm(data_loader):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     # Print epoch loss\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader):.4f}\")\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), \"lightweight_vit_classification_model.pth\")\n",
    "\n",
    "# # Evaluate the model (optional)\n",
    "# def evaluate_model(model, data_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in data_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# # If you have a separate validation set, use the evaluate_model function\n",
    "# # For now, evaluating on the same training set\n",
    "# evaluate_model(model, data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "100%|██████████| 3/3 [04:47<00:00, 95.94s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:50<00:00, 96.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 1.4130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:18<00:00, 126.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 1.3908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:14<00:00, 124.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 1.3928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:16<00:00, 105.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 1.3947\n",
      "Accuracy: 25.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a lightweight Vision Transformer model\n",
    "class LightweightViT(nn.Module):\n",
    "    def __init__(self, num_classes=4, embed_dim=128, num_heads=4, num_layers=4, mlp_dim=256):\n",
    "        super(LightweightViT, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = 16\n",
    "        self.num_patches = (224 // self.patch_size) ** 2\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # Shape: [batch_size, embed_dim, num_patches, patch_size]\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        # Transformer encoder\n",
    "        x = self.transformer_encoder(x)  # Shape: [batch_size, num_patches, embed_dim]\n",
    "\n",
    "        # Classification head\n",
    "        x = x.mean(dim=1)  # Aggregate patch embeddings\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define a custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Set paths and labels\n",
    "# image_folders = [\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/folder1\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/folder2\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/folder3\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/folder4\",\n",
    "# ]\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Assume folder1 has label 0, folder2 has label 1, etc.\n",
    "for i, folder in enumerate(image_folders):\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(i)\n",
    "\n",
    "# Define transformations and dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(image_paths, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Instantiate and move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LightweightViT(num_classes=len(image_folders))\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"lightweight_vit_classification_model.pth\")\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# If you have a separate validation set, use the evaluate_model function\n",
    "# For now, evaluating on the same training set\n",
    "evaluate_model(model, data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
