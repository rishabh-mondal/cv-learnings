{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 150])\n",
      "torch.Size([10, 5, 3])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "tensor([[ 4.0043,  2.7128, -9.3933],\n",
      "        [-5.8302, -3.5366,  3.6749],\n",
      "        [ 8.9350,  4.4561,  3.7776],\n",
      "        [-8.4856,  6.4707,  3.5814],\n",
      "        [-3.2156, -4.7981,  2.8429]])\n",
      "torch.Size([10, 5, 3])\n",
      "tensor([[147.3205,  -5.1863,  42.8768,  -0.6713, 264.2102],\n",
      "        [ 41.2147, -34.8894,  21.1526,  -5.2551,  74.4928],\n",
      "        [ 90.3215, -11.7445,  38.3033,  -8.4393, 261.7293],\n",
      "        [106.1059, -12.2584,  28.5846, -11.9671, 316.3080],\n",
      "        [152.5820, -13.1742, 135.4908, -23.6877, 174.0486],\n",
      "        [ 64.0143, -39.1483,  47.4504,  -4.1238,  94.6824],\n",
      "        [222.7346, -12.1004, 130.9173,  -8.2475, 160.0146],\n",
      "        [ 59.6297, -21.7095, 130.9173, -14.0016, 104.4292],\n",
      "        [ 45.5992, -51.3912, 128.0588, -10.4474, 274.1335],\n",
      "        [182.3969, -38.8970,  81.7518,  -4.9388, 296.4612]])\n",
      "tensor(879.7391)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#DO NOT CHANGE THE SEED\n",
    "torch.manual_seed(2)\n",
    "\n",
    "x = torch.randint(low=0,high=256,size=(1,150))\n",
    "### YOUR CODE STARTS HERE ###\n",
    "print(x.shape)\n",
    "u=x.view(10,5,3)\n",
    "x_resize = x.view(10,5,3)\n",
    "print(x_resize.shape)\n",
    "# Step2: Generate a gaussian random tensor (mean =0, variance=1) of shape (5,3) and store it in variable y1 \\\\\n",
    "# # Use torch.randn()\n",
    "# # y1=torch.randn(5,3)\n",
    "# print(y1.mean())\n",
    "# print(y1.var())\n",
    "# print(y1)\n",
    "y1 = torch.randn(5,3)\n",
    "print(y1.shape)\n",
    "# Generate a uniform random tensor (interval [-10,10)) of shape (5,3) and store it in variable y2\n",
    "\n",
    "# Use torch.rand()\n",
    "y2 = torch.rand(5,3)*20-10\n",
    "print(y2.shape)\n",
    "print(y2)\n",
    "# Perform elementwise multiplication between x_resize and y1, store it in variable mul_output1\n",
    "mul_output1 = torch.mul(x_resize,y1)\n",
    "print(mul_output1.shape)\n",
    "#fetch the last dimention element of the tensor\n",
    "final_output1,_ = torch.max(mul_output1,dim=2)\n",
    "#print the max value of final_output1\n",
    "print(final_output1)\n",
    "\n",
    "\n",
    "mul_output2 = torch.mul(x_resize,y2)\n",
    "\n",
    "final_output2,_ = torch.max(mul_output2,dim=2)\n",
    "\n",
    "# ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "print(torch.mean(final_output1+final_output2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([0.3000, 0.7000, 0.6000])\n",
      "torch.Size([3, 2])\n",
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "torch.Size([2])\n",
      "tensor([0.4033, 0.8380], requires_grad=True)\n",
      "torch.Size([3])\n",
      "tensor([ 0.3757, -0.4024, -1.6095], grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#DO NOT CHANGE THE SEED\n",
    "torch.manual_seed(0)\n",
    "\n",
    "y_true = torch.tensor([0.3, 0.7, 0.6])\n",
    "print(y_true.shape)\n",
    "print(y_true)\n",
    "m, n = 3, 2\n",
    "M = torch.randn(m, n, requires_grad=True)\n",
    "print(M.shape)\n",
    "print(M)\n",
    "v = torch.randn(n, requires_grad=True)\n",
    "print(v.shape)\n",
    "print(v)\n",
    "#calculate the matrix vector product between M and v and store it in y_pred\n",
    "y_pred = torch.matmul(M,v)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "### YOUR CODE STARTS HERE ###\n",
    "# y_pred = ___\n",
    "### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True])\n",
      "tensor(2.0324, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Next, implement a custom loss function in PyTorch that computes the mean squared error (MSE) \n",
    "# between two tensors (y_true, y_pred), but only for elements where the target tensor y_true is greater than 0.5. Save the calculated MSE loss in a variable named \"loss\".\n",
    "\n",
    "# *(Hint: First create a mask on y_true and then multiply the mask with the loss)*\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss,self).__init__()\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mask=y_true>0.5\n",
    "        print(mask)\n",
    "        squared_diff = (y_pred-y_true)**2\n",
    "        masked_squared_diff = squared_diff[mask]\n",
    "        # For calculating the mean, take the sum of masked_squared_diff and divide by 3.\n",
    "        loss = torch.sum(masked_squared_diff)/3\n",
    "        return loss\n",
    "    \n",
    "\n",
    "loss_fn = CustomMSELoss()\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print(loss)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Gradient Means:  tensor(1.9638)\n"
     ]
    }
   ],
   "source": [
    "### DO NOT CHANGE ###\n",
    "# Compute gradients using autograd\n",
    "loss.backward(retain_graph=True)  # This computes gradients for all tensors that have requires_grad=True\n",
    "\n",
    "### YOUR CODE STARTS HERE ###\n",
    "grad_M = torch.autograd.grad(loss, M, retain_graph=True)[0]\n",
    "grad_v = torch.autograd.grad(loss, v, retain_graph=True)[0]\n",
    "### YOUR CODE ENDS HERE ###\n",
    "\n",
    "print(\"Sum of Gradient Means: \", grad_M.mean()+grad_v.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([0.3000, 0.7000, 0.6000])\n",
      "torch.Size([3, 2])\n",
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "torch.Size([2])\n",
      "tensor([0.4033, 0.8380], requires_grad=True)\n",
      "torch.Size([3])\n",
      "tensor([ 0.3757, -0.4024, -1.6095], grad_fn=<MvBackward0>)\n",
      "tensor(3.0486, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# This computes gradients for all tensors that have requires_grad=True\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Calculate gradients with respect to M and v\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m grad_M \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m grad_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, v, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of Gradient Means: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grad_M\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m grad_v\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# DO NOT CHANGE THE SEED\n",
    "torch.manual_seed(0)\n",
    "\n",
    "y_true = torch.tensor([0.3, 0.7, 0.6])\n",
    "print(y_true.shape)\n",
    "print(y_true)\n",
    "m, n = 3, 2\n",
    "M = torch.randn(m, n, requires_grad=True)\n",
    "print(M.shape)\n",
    "print(M)\n",
    "v = torch.randn(n, requires_grad=True)\n",
    "print(v.shape)\n",
    "print(v)\n",
    "\n",
    "# Calculate the matrix-vector product between M and v and store it in y_pred\n",
    "y_pred = torch.matmul(M, v)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "\n",
    "# Define a custom MSE loss function\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        mask = y_true > 0.5\n",
    "        squared_diff = (y_pred - y_true) ** 2\n",
    "        masked_squared_diff = squared_diff[mask]\n",
    "        # Use mask.sum() to get the number of valid elements for the mean calculation\n",
    "        loss = torch.sum(masked_squared_diff) / mask.sum()\n",
    "        return loss\n",
    "\n",
    "loss_fn = CustomMSELoss()\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print(loss)\n",
    "\n",
    "# Compute gradients using autograd\n",
    "loss.backward()  # This computes gradients for all tensors that have requires_grad=True\n",
    "\n",
    "# Calculate gradients with respect to M and v\n",
    "grad_M = torch.autograd.grad(loss, M, retain_graph=True)[0]\n",
    "grad_v = torch.autograd.grad(loss, v, retain_graph=True)[0]\n",
    "\n",
    "print(\"Sum of Gradient Means: \", grad_M.mean() + grad_v.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
