{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 00:51:05.756591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-13 00:51:05.780993: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-13 00:51:05.781069: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-13 00:51:05.797705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-13 00:51:06.753597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████| 6/6 [02:44<00:00, 27.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 5.3261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:42<00:00, 27.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:46<00:00, 27.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:49<00:00, 28.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 1.4407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:21<00:00, 43.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 1.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:16<00:00, 52.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 1.3212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:09<00:00, 51.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 1.1944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:23<00:00, 53.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 1.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [08:02<00:00, 80.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [07:26<00:00, 74.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.7778\n",
      "Accuracy: 49.79%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Set paths and labels\n",
    "image_folders = [\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Facebook\",\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "    \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Telegram\",\n",
    "]\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Assume folder1 has label 0, folder2 has label 1, etc.\n",
    "for i, folder in enumerate(image_folders):\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(i)\n",
    "\n",
    "# Define transformations and dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(image_paths, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Load the ViT model and move it to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"vit_classification_model.pth\")\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# If you have a separate validation set, use the evaluate_model function\n",
    "# For now, evaluating on the same training set\n",
    "evaluate_model(model, data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torchvision import transforms\n",
    "# from transformers import DeiTForImageClassification, DeiTFeatureExtractor\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define a custom dataset class\n",
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, image_paths, labels, transform=None):\n",
    "#         self.image_paths = image_paths\n",
    "#         self.labels = labels\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "#         label = self.labels[idx]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, label\n",
    "\n",
    "# # Set paths and labels\n",
    "# image_folders = [\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Facebook\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Instagram\",\n",
    "#     \"/home/rishabh.mondal/Brick-Kilns-project/albk_rishabh/albk_v2/YOLO_LOCALIZATION/cv-learnings/coco_ankiita/SD2Q/Train/Telegram\",\n",
    "# ]\n",
    "\n",
    "# image_paths = []\n",
    "# labels = []\n",
    "\n",
    "# # Assume folder1 has label 0, folder2 has label 1, etc.\n",
    "# for i, folder in enumerate(image_folders):\n",
    "#     for img_name in os.listdir(folder):\n",
    "#         img_path = os.path.join(folder, img_name)\n",
    "#         image_paths.append(img_path)\n",
    "#         labels.append(i)\n",
    "\n",
    "# Define transformations and dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# dataset = ImageDataset(image_paths, labels, transform=transform)\n",
    "# data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Load the DeiT-Tiny model and move it to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# from transformers import DeiTForImageClassification\n",
    "\n",
    "# # Clear cache and retry\n",
    "# DeiTForImageClassification.from_pretrained('facebook/deit-tiny-patch16-224', force_download=True)\n",
    "\n",
    "# model.classifier = torch.nn.Linear(model.config.hidden_size, len(image_folders))  # Adjusting output layer to match number of classes\n",
    "# model.to(device)\n",
    "\n",
    "# # Define the optimizer and loss function\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# model.train()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels in tqdm(data_loader):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs.logits, labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     # Print epoch loss\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader):.4f}\")\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), \"deit_tiny_classification_model.pth\")\n",
    "\n",
    "# # Evaluate the model (optional)\n",
    "# def evaluate_model(model, data_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in data_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.logits, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# # If you have a separate validation set, use the evaluate_model function\n",
    "# # For now, evaluating on the same training set\n",
    "# evaluate_model(model, data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 00:21:50.224955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79090 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n",
      "2024-08-13 00:21:50.226682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79090 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2024-08-13 00:21:50.228045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79090 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-08-13 00:21:50.229414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 79090 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n",
      "2024-08-13 00:21:51.184021: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:510] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-12.3\n",
      "  /usr/local/cuda\n",
      "  /opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-08-13 00:21:51.184106: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:548] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2024-08-13 00:21:51.185496: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:207] INTERNAL: Generating device code failed.\n",
      "2024-08-13 00:21:51.188835: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: JIT compilation failed.\n",
      "2024-08-13 00:21:51.188904: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define your custom model\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_crossvit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m, in \u001b[0;36mcreate_crossvit_model\u001b[0;34m(input_shape, num_classes, patch_size, embedding_dim)\u001b[0m\n",
      "\u001b[1;32m     63\u001b[0m embedded_patches \u001b[38;5;241m=\u001b[39m EmbedPatchesLayer(embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim)(patches)\n",
      "\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Positional encoding\u001b[39;00m\n",
      "\u001b[0;32m---> 66\u001b[0m encoded_patches \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncodingLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_patches\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Flatten and add dense layers\u001b[39;00m\n",
      "\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(encoded_patches)\n",
      "\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rishabh_sat/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mPositionalEncodingLayer.build\u001b[0;34m(self, input_shape)\u001b[0m\n",
      "\u001b[1;32m     11\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m     12\u001b[0m d_model \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_positional_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mPositionalEncodingLayer._get_positional_encodings\u001b[0;34m(self, seq_len, d_model)\u001b[0m\n",
      "\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_positional_encodings\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq_len, d_model):\n",
      "\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Generate position indices\u001b[39;00m\n",
      "\u001b[1;32m     17\u001b[0m     position \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrange(seq_len, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)[:, tf\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "\u001b[0;32m---> 18\u001b[0m     div_term \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(tf\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m0\u001b[39m, d_model, \u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000.0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m d_model))\n",
      "\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Compute the positional encoding\u001b[39;00m\n",
      "\u001b[1;32m     21\u001b[0m     pos_enc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([\n",
      "\u001b[1;32m     22\u001b[0m         tf\u001b[38;5;241m.\u001b[39msin(position \u001b[38;5;241m*\u001b[39m div_term),\n",
      "\u001b[1;32m     23\u001b[0m         tf\u001b[38;5;241m.\u001b[39mcos(position \u001b[38;5;241m*\u001b[39m div_term)\n",
      "\u001b[1;32m     24\u001b[0m     ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log] name: "
     ]
    }
   ],
   "source": [
    "# Define your custom model\n",
    "model = create_crossvit_model(input_shape=(224, 224, 7), num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
